\documentclass[12pt]{article}

\usepackage{amssymb,amsmath,amsfonts,eurosym,geometry,ulem,graphicx,caption,color,setspace,sectsty,comment,footmisc,caption,natbib,pdflscape,subfigure,array,hyperref}

\normalem

\onehalfspacing
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}{Proposition}
\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}

\newtheorem{hyp}{Hypothesis}
\newtheorem{subhyp}{Hypothesis}[hyp]
\renewcommand{\thesubhyp}{\thehyp\alph{subhyp}}

\newcommand{\red}[1]{{\color{red} #1}}
\newcommand{\blue}[1]{{\color{blue} #1}}

\newcolumntype{L}[1]{>{\raggedright\let\newline\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\arraybackslash\hspace{0pt}}m{#1}}


\geometry{left=1.0in,right=1.0in,top=1.0in,bottom=1.0in}

\begin{document}

\begin{titlepage}
\title{A review of High Dimensional Nonlinear Data Dimension Reduction Methods
\thanks{Electronic address: \texttt{jaguirre31@gatech.edu}; Corresponding author}}
\author{Jacob Aguirre, Ruirui Ma, Shrey Patel}
\date{\today}
\maketitle
\begin{abstract}
\noindent Massive, high-dimensional data sets are already appearing in a variety of sectors of modern research, posing new obstacles. These high-dimensional data sets can typically be modeled as point clouds in a $D-$dimensional space, such that it's concentrated near a $d-$dimensional manifold. Thus, we are able to visualize this data as a low-dimensional representation. The well-known curse of dimensionality in machine learning suggests that a huge amount of training data is necessary in order to obtain a given prediction accuracy. Unless additional assumptions are established, picture and signal recovery requires a significant number of observations to recover a high-dimensional vector. Owing to the fact there exist rich localized constants, global symmetry, repeated patterns, or redundant sampling, many real-world data sets show low-dimensional geometric structures. This study seeks to investigate these local structures on a variety of simulated and real data-sets, and discover which properties contribute to certain models performing well or under performing. As a result, we're attempting to investigate low-dimensional geometric patterns in data sets in order to extract features, forecast data, and recover signals.  \\
\vspace{0in}\\
\noindent\textbf{Keywords:} High-Dimensional Data, Machine Learning, Statistical Inference\\
\vspace{0in}\\
\noindent\textbf{JEL Codes:} C15, C44, C45\\

\bigskip
\end{abstract}
\setcounter{page}{0}
\thispagestyle{empty}
\end{titlepage}
\pagebreak \newpage




\doublespacing


\section{Background} \label{sec:introduction}

\hspace{5mm}The $21^{st}$ century presents new challenges in the realm of statistical inference and data mining. High dimensional data sets are arising more often, which contributes to challenges in attempting to understand and draw conclusions from such data. For the purposes of image and signal recovery, a large amount of technical expertise is needed in order to recover a high-dimensional vector we're interested in [1].

Machine learning can assist us in comprehending enormous amounts of data and making judgments based on that information. However, in order to do so, we must first figure out how to extract the data's information effectively (and efficiently). Often times, we can visualize these data sets on a lower dimensional manifold. Consider the classic example of images, text files, or even sound waves. These can be expressed as a vector $x\in\mathbb{R}^n$. Thus, each element in this vector would correspond to a pixel, word, or signal, respectively.

In this work, we will seek to focus on 4 different methods. These are Isomap, T-SNE, Diffusion Maps, and Local Linear Embedding [2]. Each presents it's own unique methodology, pros, and cons. Within this report, we will, of course, provide the necessary background and prerequisite knowledge needed to understand each method. We will make sure to focus specifically on the applications, how well our methods worked, and the limitations with each. 

The rest of this paper is as follows: Section 2 introduces background of existing literature of nonlinear dimension reduction methods; Section 3 is split into 4 subsections, each discussing a different algorithm and the corresponding methodology. Section 4 provides a commentary of the results, as well as supplying an analytic approach to decide whether our hypothesis was correct for nonlinear dimension reduction methods; Section 5 presents concluding remarks and avenues of future research. Attached is an appendix, which will have all the graphs, charts, and tables we created.


\section{Methodology} \label{sec:methodology}

\hspace{5mm}Often times, we are presented with a data set and told to find causal inferences within. We seek to group data, make hypotheses, draw conclusions, and present our results. Formally, a dataset can be given by the following: $X:=[x_{ij}]|x_{ij}\in f_j,\forall i\in[1,|S|],\forall j\in[1,|F|]$ [3]. This is important to know, as most of the data sets we used follow this description. Consider a dataset $X$ with $n$ observations and $p$ variables. Thus, $X$ is a $n$x$p$ matrix. It follows that each row corresponds to an observation, and each column in this matrix is a unique feature. 

Each of a data set's nominal features can be enumerated, or mapped to an element of $\mathbb{N}$. A collection of vectors in the $\mathbb{R}^n$ could then be used to express such a set. High dimensional data is often unique, and different from some of the data sets we normally encounter. Consider that with a high dimensional data, the number of samples is high, alongside the number of features.

We now consider the case of facial images. Often times for image reconstruction or computational inverse problems, the data set will require us to center, or normalize, the data. This equates to us taking the mean of the data, and subtracting it from the original dataset. Formally, this is written as $\textbf{X}'_j=\textbf{X}_j-\mu_j\textbf{1}_n$ where $[\textbf{X}]_{n x f}$ is a data set, $\textbf{X}_j$ is the $j^{th}$ column of $\textbf{X}$, and $\mu_j$ is the mean of $\textbf{X}_j$. Further, $\textbf{1}_n$is a column vector of 1's. Thus, $\textbf{X'}_j$ is a normalized data set where each column has mean 0.


We now seek to explain the Covariance Matrix of features for a centered data set. Consider $\textbf{X}$ to be a data set, $\textbf{X'}$ to be the same data set $\textbf{X}$ except it has the features centered. Formally, this would be written as $\textbf{X}'=\textbf{HX}$. Also, consider the matrix $\textbf{X}'_j$ to be the $j^{th}$ feature column of the centered data set.

We accept the following identity to be true, and do not prove it in the appendix. Given a data set $\textbf{X}$, we have that the covariance between pairs of features is given by a matrix $\sum_X=[\sigma_{xy}]_{nxn}=\frac{1}{n}\textbf{X}^T\textbf{HX}$. This assumption will allow us to better easily implement the algorithms in the paper. 

\subsection{Topology and Manifolds}

Formally, we can consider a manifold to be a set $M$ to be a $n-$dimensional topological manifold $\iff$ $\forall p\in M,p\in U_p$ where $U_p$ is an open set homeomorphic to an open set $V_p$ in the Euclidean space $\mathbb{R}^n.$ The properties of a manifold and it's Atlas are important when we discuss our nonlinear dimension techniques, with emphasis for Local Linear Embedding.

For example, consider two sets $A$ and $B$, such that they are both two seperate topological objects, and a function $\psi:A\to B$ would be considered an "embedding" of $A\to B$. We will not prove any further assumptions, and any such statements in the report are left to the reader as an exercise. 


\section{Local Linear Embedding} \label{sec:Local Linear Embedding}

\subsection{Motivation}

\subsection{Approach}

\subsection{Results}

\section{Diffusion Map} \label{sec: Diffusion Map}

\subsection{Motivation}

\subsection{Approach}

\subsection{Results}

\section{T-SNE} \label{sec: T-SNE}
\subsection{Motivation}

\subsection{Approach}

\subsection{Results}

\section{Isomap} \label{sec: Isomap}


\subsection{Motivation}

\subsection{Approach}

\subsection{Results}


\hspace{5mm}The Isomap Method.

\subsubsection{Notation}
\hspace{5mm}\text We begin by defining the notation that will be utilized within the algorithm to compute an Isomap. Consider a matrix $\textbf{H}$ named the centering matrix. This is only the centering matrix after a multiplication by a given matrix $\textbf{X}$. This produces an effect of subtracting the mean of the components from each in $\textbf{X}$. Formally, $\textbf{H}$ is given by 

\text{$\textbf{H}=\textbf{I}_n-\frac{1}{n}\textbf{11}^T,$where}

\text{$\textbf{I}_n$ is the Identity matrix and $\textbf{1}$ is the column vector filled with $1's$.}

\text{}

\subsection{Results}

\newpage

\section{Discussion} \label{sec:discussion}

\hspace{5mm}Many teens in the United States report being exposed to e-cigarette marketing messages via a number of outlets. Ads for e-cigarettes may impact teenage beginning of e-cigarette usage. According to the findings of this study, exposure to e-cigarette ads can also impact teenagers' risk perception of smoking cigarettes, which is a strong predictor of smoking start (Roditis et al., 2016; Song et al., 2009). Youth who've never-smoked and were exposed to e-cigarette commercials reported considerably decreased perceived hazards of smoking cigarettes. Regulating the platforms and material of e-cigarette marketing should be examined in order to reduce teenage exposure and the potentially negative impacts on nonsmokers. 


\subsection{Limitations}
\hspace{5mm}For this brief report, we sought to discuss the topic of nonlinear dimensionality reduction. First, we effectively reduced, displayed, and learnt from well-known data sets by focusing on linear algorithms and their ability to extract features that optimize variance in a data set. In addition, we illustrated how these techniques would not be able to decrease data sets with a nonlinear distribution.

Thus, this caused for us to seek out nonlinear dimension reduction techniques. For this, we utilized the following: T-SNE, Isomap, Diffusion Maps, and Local Linear Embedding (LLE). We found that each method had their merits and difficulties. For example, the isomap represents a step forward in manifold learning and was a widely used dimensionality reduction method until recently. Currently, Uniform Manifold Approximation and Projection (UMAP) had begun to take over the literature. In practice, its relevance can be seen in the large number of machine learning libraries, languages, and computational environments that use it, as well as the large amount of research done by numerous authors attempting to apply, improve, or expand it.

We end by concluding that this study was overall quite successful. We showed a multitude of different techniques, how well they will perform or fail on given data sets, and what to consider when using them. It's always important to note that within the realm of data science and statistical inference, a "one method fits all" approach is likely to fail. You must always be aware of the particularities within your dataset, and constantly tryout new methods to explore and find causal inference within. This 

\newpage
\begin{thebibliography}{}
\bibitem{First Entry}Fannjiang, Albert, and Wenjing Liao. "Coherence pattern–guided compressive sensing with unresolved grids." SIAM Journal on Imaging Sciences 5.1 (2012): 179-202.

\bibitem{scikit learn}scikit learn, “Svm: Maximum margin separating hyperplane.” [Online].Available: http://scikit-learn.org/stable/auto$_$examples/svm/plot$_$separating$_$hyperplane.html.


\bibitem{UCI machine learning}M. Lichman, “UCI machine learning repository,” 2013

\bibitem{bioinformatics}P. Baldi and S. Brunak, Bioinformatics: The Machine Learning Approach. A
Bradford book, A Bradford Book, 2001.

\bibitem{geometric structure}J. Wang, Geometric Structure of High-Dimensional Data and Dimensionality
Reduction. Springer Berlin Heidelberg, 2012.

\bibitem{definitions}S. S. A. H. Renear and K. M. Wickett, “Definitions of dataset in the scientific
and technical literature,” ASIS&T 2010, 2010

\bibitem{algorithms for manifolds}L. Cayton, “Algorithms for manifold learning,” Univ. of California at San
Diego Tech. Rep, pp. 1–17, 2005.

\bibitem{MDS}T. Cox and M. A. A. Cox, Multidimensional scaling. Boca Raton, FL, USA:
CRC Press, 2000.

\bibitem{graph theory}W. Mayeda, Graph Theory. John Wiley & Sons, 1972.

\bibitem{elements of machine}P. Langley, Elements of Machine Learning. Machine Learning Series, Morgan
Kaufmann, 1996.

\bibitem{semi-supervised learning}X. Zhu and A. B. Goldberg, “Introduction to semi-supervised learning,” Synthesis lectures on artificial intelligence and machine learning, vol. 3, no. 1,
pp. 1–130, 2009


\end{thebibliography}


\section{Funding Statement}
Funding for this study was provided by the Georgia Institute of Technology Presidential Undergraduate Research Award. Georgia Tech held no role in the study design, collection, analysis or interpretation of the data, writing the manuscript, or the decision to submit the paper for publication. No co-author held any accompanying grants for this work. 

\section{Disclaimer}
The findings and conclusions in this report are those of the authors and do not necessarily represent the official position of the Georgia Institute of Technology or any of its affiliated institutions or agencies. This article was prepared while Jacob Aguirre was an undergraduate student at the Georgia Institute of Technology, School of Economics and Mathematics. 

\end{document}